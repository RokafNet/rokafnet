{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from glob import glob\n",
    "import IPython.display as ipd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/rokafnet/.venv/lib64/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from scipy.io import wavfile\n",
    "import noisereduce as nr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fix Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)  # type: ignore\n",
    "    torch.backends.cudnn.deterministic = True  # type: ignore\n",
    "    torch.backends.cudnn.benchmark = True  # type: ignore\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'model': 'openai/whisper-tiny',\n",
    "    'sr': 16000,\n",
    "    'noise_file_path': '../files/noise.wav'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = '/mnt/elice/dataset/train/'\n",
    "TEST_PATH = '/mnt/elice/dataset/test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f'{TRAIN_PATH}/texts.csv', index_col=False)\n",
    "submission = pd.read_csv(f'sample_submission.csv', index_col=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text, remove_space=True):\n",
    "    text = re.sub(r'[!\"#$%&\\'()*+,-./:;<=>?@\\[\\]^_\\`{|}~\\\\\\\\]','', text)\n",
    "    if remove_space:\n",
    "        text = ''.join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Cleaning (remove punctuations)\n",
    "df['text'] = df['text'].apply(lambda x: clean_text(x, False))\n",
    "\n",
    "# remove outlier data\n",
    "df = df[df['filenames'] != 'audio_5497.wav']\n",
    "\n",
    "if not os.path.exists('preprocess'):\n",
    "    os.mkdir('preprocess')\n",
    "\n",
    "df.to_csv('preprocess/clean_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split long/short dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def split_dataframe(df, df_name, is_train=True):\n",
    "    df_long = []\n",
    "    df_short = []\n",
    "\n",
    "    for idx, row in tqdm(df.iterrows()):\n",
    "        if is_train:\n",
    "            path = TRAIN_PATH + row['filenames']\n",
    "        else:\n",
    "            path = row['path']\n",
    "        wav, fs = librosa.load(path)\n",
    "        length = len(wav)/fs\n",
    "\n",
    "        if length >= 30:\n",
    "            df_long.append(row)\n",
    "        else:\n",
    "            df_short.append(row)\n",
    "\n",
    "    df_long = pd.DataFrame(df_long, columns=df.columns)\n",
    "    df_short = pd.DataFrame(df_short, columns=df.columns)\n",
    "\n",
    "    df_long.to_csv(f'preprocess/long_{df_name}.csv', index=False)\n",
    "    df_short.to_csv(f'preprocess/short_{df_name}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13999it [00:48, 287.87it/s]\n",
      "6000it [00:23, 255.25it/s]\n"
     ]
    }
   ],
   "source": [
    "split_dataframe(df, 'df')\n",
    "split_dataframe(submission, 'test', False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocess & Train Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperTokenizer,  WhisperFeatureExtractor\n",
    "from transformers import WhisperProcessor\n",
    "from scipy.io import wavfile\n",
    "\n",
    "# load feature extractor and tokenizer\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(CFG['model'])\n",
    "tokenizer = WhisperTokenizer.from_pretrained(CFG['model'], language=\"Korean\", task=\"transcribe\")\n",
    "\n",
    "_, noise_array = wavfile.read(CFG[\"noise_file_path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(batch):\n",
    "    audio = batch['audio']\n",
    "    reduced_noise_audio = nr.reduce_noise(y=audio['array'], sr=CFG['sr'], y_noise = noise_array)\n",
    "\n",
    "    # raw form(reduced_noise_audio) -> log-Mel spectrogram\n",
    "    batch['input_features'] = feature_extractor(reduced_noise_audio, sampling_rate=audio['sampling_rate']).input_features[0]\n",
    "    \n",
    "    # target text -> label ids(by tokenizer)\n",
    "    batch['labels'] = tokenizer(batch['transcripts']).input_ids\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "from datasets import Audio\n",
    "\n",
    "def create_train_datasets(df, dir_name='dataset'):\n",
    "    # create dataset from csv\n",
    "    ds = Dataset.from_dict({\"audio\": [f'{TRAIN_PATH}/{file_path}' for file_path in df[\"filenames\"]],\n",
    "                        \"transcripts\": [text for text in df[\"text\"]]}).cast_column(\"audio\", Audio(sampling_rate=CFG['sr']))\n",
    "\n",
    "    # train/valid split\n",
    "    train_valid = ds.train_test_split(test_size=0.2)\n",
    "    train_valid_dataset = DatasetDict({\n",
    "        \"train\": train_valid[\"train\"],\n",
    "        \"valid\": train_valid[\"test\"]})\n",
    "    \n",
    "    train_valid_dataset = train_valid_dataset.map(prepare_dataset, remove_columns = train_valid_dataset.column_names['train'], num_proc=4)\n",
    "\n",
    "    if not os.path.exists(dir_name):\n",
    "        os.mkdir(dir_name)\n",
    "        \n",
    "    train_valid_dataset.save_to_disk(dir_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4): 100%|██████████| 11086/11086 [09:35<00:00, 19.27 examples/s]\n",
      "Map (num_proc=4): 100%|██████████| 2772/2772 [02:26<00:00, 18.96 examples/s]\n",
      "Saving the dataset (22/22 shards): 100%|██████████| 11086/11086 [00:15<00:00, 721.93 examples/s]\n",
      "Saving the dataset (6/6 shards): 100%|██████████| 2772/2772 [00:03<00:00, 700.25 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# create_train_datasets(df)\n",
    "\n",
    "# create long/short train dataset from csv files\n",
    "# long_df = pd.read_csv('preprocess/long_df.csv', index_col=False)\n",
    "short_df = pd.read_csv('preprocess/short_df.csv', index_col=False)\n",
    "\n",
    "# create_train_datasets(long_df, dir_name='dataset_long')\n",
    "create_train_datasets(short_df, dir_name='dataset_short')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_test_dataset(batch):\n",
    "    audio = batch['audio']\n",
    "    reduced_noise_audio = nr.reduce_noise(y=audio['array'], sr=CFG['sr'], y_noise = noise_array)\n",
    "\n",
    "    # raw form(reduced_noise_audio) -> log-Mel spectrogram\n",
    "    batch['input_features'] = feature_extractor(reduced_noise_audio, sampling_rate=audio['sampling_rate']).input_features[0]\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "from datasets import Audio\n",
    "\n",
    "def create_test_dataset(df, dir_name='dataset_test'):\n",
    "    # create dataset from csv\n",
    "    test_dataset = Dataset.from_dict({\"audio\": [file_path for file_path in df[\"path\"]]})\n",
    "    test_dataset = test_dataset.cast_column(\"audio\", Audio(sampling_rate=CFG['sr']))\n",
    "    sampling_rate = test_dataset.features['audio'].sampling_rate\n",
    "\n",
    "    # test data preprocess\n",
    "    test_dataset = test_dataset.map(prepare_test_dataset, remove_columns = test_dataset.column_names, num_proc=4)\n",
    "\n",
    "    # save test dataset\n",
    "    if not os.path.exists(dir_name):\n",
    "        os.mkdir(dir_name)\n",
    "        \n",
    "    test_dataset.save_to_disk(dir_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4): 100%|██████████| 6000/6000 [05:19<00:00, 18.77 examples/s]\n",
      "Saving the dataset (12/12 shards): 100%|██████████| 6000/6000 [00:02<00:00, 2208.39 examples/s]\n"
     ]
    }
   ],
   "source": [
    "create_test_dataset(submission)\n",
    "\n",
    "# create long/short test dataset from csv files\n",
    "# long_test = pd.read_csv('preprocess/long_test.csv', index_col=False)\n",
    "# short_test = pd.read_csv('preprocess/short_test.csv', index_col=False)\n",
    "\n",
    "# create_test_dataset(long_test, dir_name='dataset_long_test')\n",
    "# create_test_dataset(short_test, dir_name='dataset_short_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
