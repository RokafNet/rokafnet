{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets >=2.6.\n",
    "%pip install git+https://github.com/huggingface/transformers\n",
    "%pip install jiwer\n",
    "%pip install accelerate -U\n",
    "%pip install transformers[torch]\n",
    "%pip install noisereduce\n",
    "%pip install wandb\n",
    "%pip install evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from glob import glob\n",
    "import IPython.display as ipd\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from scipy.io import wavfile\n",
    "\n",
    "from datasets import load_from_disk\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "from transformers import WhisperTokenizer,  WhisperFeatureExtractor, WhisperProcessor\n",
    "from transformers import WhisperForConditionalGeneration\n",
    "from transformers import Seq2SeqTrainingArguments\n",
    "from transformers import AdamW, get_cosine_schedule_with_warmup\n",
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "import evaluate\n",
    "import re\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fix Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)  # type: ignore\n",
    "    torch.backends.cudnn.deterministic = True  # type: ignore\n",
    "    torch.backends.cudnn.benchmark = True  # type: ignore\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = '/mnt/elice/dataset/train/'\n",
    "TEST_PATH = '/mnt/elice/dataset/test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'model': 'openai/whisper-tiny',\n",
    "    'sr': 16000,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_valid_dataset = load_from_disk('dataset_short')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # 인풋 데이터와 라벨 데이터의 길이가 다르며, 따라서 서로 다른 패딩 방법이 적용되어야 한다. 그러므로 두 데이터를 분리해야 한다.\n",
    "        # 먼저 오디오 인풋 데이터를 간단히 토치 텐서로 반환하는 작업을 수행한다.\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # Tokenize된 레이블 시퀀스를 가져온다.\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # 레이블 시퀀스에 대해 최대 길이만큼 패딩 작업을 실시한다.\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # 패딩 토큰을 -100으로 치환하여 loss 계산 과정에서 무시되도록 한다.\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # 이전 토크나이즈 과정에서 bos 토큰이 추가되었다면 bos 토큰을 잘라낸다.\n",
    "        # 해당 토큰은 이후 언제든 추가할 수 있다.\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processor, tokenizer, feature extractor 로드\n",
    "processor = WhisperProcessor.from_pretrained(CFG['model'], language=\"Korean\", task=\"transcribe\")\n",
    "tokenizer = WhisperTokenizer.from_pretrained(CFG['model'], language=\"Korean\", task=\"transcribe\")\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(CFG['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 콜레이터 초기화\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load('cer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, remove_space=True):\n",
    "    text = re.sub(r'[!\"#$%&\\'()*+,-./:;<=>?@\\[\\]^_\\`{|}~\\\\\\\\]','', text)\n",
    "    if remove_space:\n",
    "        text = ''.join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # pad_token을 -100으로 치환\n",
    "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "\n",
    "    # metrics 계산 시 special token들을 빼고 계산하도록 설정\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    # 문장부호 + 띄어쓰기 제거하고 계산\n",
    "    pred_str = [clean_text(text) for text in pred_str]\n",
    "    label_str = [clean_text(text) for text in label_str]\n",
    "\n",
    "    cer = metric.compute(predictions=pred_str, references=label_str)\n",
    "    \n",
    "    return {\"cer\": cer}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrained Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WhisperForConditionalGeneration.from_pretrained(CFG['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(language='korean', task='transcribe')\n",
    "model.config.suppress_tokens = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    torch_compile=True, # for optimize code\n",
    "\n",
    "    output_dir=\"checkpoint\",\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=1e-5,\n",
    "    warmup_steps=500,\n",
    "    num_train_epochs=10,\n",
    "    optimizers=(AdamW, get_cosine_schedule_with_warmup),\n",
    "\n",
    "    fp16=True,\n",
    "\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=560,\n",
    "    per_device_eval_batch_size=4,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,\n",
    "    dataloader_num_workers=8,\n",
    "    \n",
    "    save_strategy='steps',\n",
    "    save_steps=560,\n",
    "    logging_steps=25,\n",
    "    report_to=[\"wandb\"],\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"cer\",\n",
    "    greater_is_better=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=train_valid_dataset[\"train\"],\n",
    "    eval_dataset=train_valid_dataset[\"valid\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save checkpoint\n",
    "MODEL_PATH = 'model'\n",
    "trainer.save_model(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set wandb to save all codes, weights, and results\n",
    "wandb.save(\"*.py\")\n",
    "wandb.save(\"*.ipynb\")\n",
    "\n",
    "wandb.save(\"*.pt\")\n",
    "wandb.save(\"*.pth\")\n",
    "wandb.save(\"*.hdf5\")\n",
    "\n",
    "wandb.save(\"*.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(language='korean', task='transcribe')\n",
    "model.config.suppress_tokens = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  load test dataset from local storage\n",
    "test_dataset = load_from_disk('dataset_test')\n",
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_args = Seq2SeqTrainingArguments(\n",
    "    torch_compile=True, # for optimize code\n",
    "\n",
    "    output_dir=\"repo_name\",\n",
    "\n",
    "    per_device_eval_batch_size=32,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,\n",
    "    dataloader_num_workers=8,\n",
    "    \n",
    "    logging_steps=25,\n",
    "    # report_to=[\"wandb\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# inference\n",
    "test_trainer = Seq2SeqTrainer(args=test_args,\n",
    "                              model=model)\n",
    "pred = test_trainer.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = tokenizer.batch_decode(pred.predictions, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('sample_submission.csv')\n",
    "submission['text'] = text\n",
    "submission.to_csv('raw_submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
